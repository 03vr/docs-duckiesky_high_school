# SLAM Assignment {#localization-slam-assignment status=ready}

The assignment is slightly different than the localization project due to some computational constraints. Unfortunately, SLAM is not fast enough to run in real time on board the raspberry pi. Instead, you will implement SLAM to run on some pre-recorded flight data (lists of keypoints, descriptors, and IR data). Your SLAM will run offline on the data, and you can judge the correctness of your implementation using animate_particle_filter.py to view the animation of the flight. We will provide you with an animation produced by our solution code for comparison.

If you try to run your SLAM program offline on the drone, you will find that the program takes up to 15 minutes to run! Instead, we recommend that you use the department machines to develop your solution, as they have opencv, numpy, and matplotlib pre-installed.

We will provide you with the following files:

    slam.py
    slam_helper.py
    utils.py
    map_data.txt
    pose_data.txt
    animate_particle_filter.py
    animation.gif
    pidrone_vision_instructions.md

slam.py and utils.py contain fully implemented helper code that you are welcome to read but do not need to edit. You may, however, edit the number of particles in slam.py to experiment with the speed/accuracy tradeoff for FastSLAM. You should find that 15 particles are plenty.

utils contains add_landmark and update_landmark as promised, as well as some other helper code.

map_data.txt contains data from a previous flight. It stores keypoints, descriptors, and IR data from each camera frame. The helper code included in slam.py will read the data from map_data, create a FastSLAM object imported from slam_helper.py, and run the data through your SLAM implementation.

pose_data.txt will hold the estimated pose of each particle at each camera frame. slam_helper.py will write these poses as it processes the flight data. These poses are computed by your implementation of SLAM. You can use animate_particle_filter.py to read the poses and view the animation from your SLAM, just remember the set the number of particles in animate_particle_filter.py to the same number in slam.py!

animation.gif is the animation generated by our solution code with which you can compare your own animations.

The only thing left for you to do is implement the missing parts of the slam_helper file, which are indicated with TODOs. The intended functionality of each missing method is indicated in the docstrings.

## Testing
You should develop your SLAM implementation on the department machines using the provided helper code. When you want to test your implementation, you should run animate_particle_filter.py and see if your animation closely follows that from the solution code.

Once your implementation is working well and you have shown it to a TA for checkoff, you are ready to test your implementation on the drone! Unfortunately, our raspberry pi's are not powerful enough to run FastSLAM and their own PID controllers, etc in real time. Trying to do so will result in a drone flying haywire and SLAM that is too slow to produce accurate poses. We have a couple options to work around this problem:

 1. Network in an offboard computer computer to run your SLAM programs while the drone is flying. This option is ideal, but only feasible if you have access to a computer running Ubuntu 14.04 with ROS kinetic installed.
 2. Sequentially perform SLAM and localization. This involves flying the drone over the area you want to map and recording the data received by the camera (keypoints and descriptors) during the flight. The drone then lands and runs the collected data through FastSLAM to build a map. Finally, the drone flies and runs a modified version of localization which uses the map produced by SLAM to localize in real time.

You should follow the directions in PiDrone Vision Instructions to test your SLAM using option 2  (this is onboard/offline SLAM). For option 1, you are welcome to use an ubuntu computer to test your implementation in real time (this is offboard/online SLAM) following the directions in PiDrone Vision Instructions. We ask that you complete the checkoff below before asking to use one of our ubuntu computers.

## Checkoff

The checkoff for this project is simple, run your slam_helper.py implementation on the sample data for a TA. Show the TA the corresponding animation. If you would like to test your implementation in real time using an offboard computer, let your TA know during checkoff, and they will show you how to run SLAM offboard with your implementation.
